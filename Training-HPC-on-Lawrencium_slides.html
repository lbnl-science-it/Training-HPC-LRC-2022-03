<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Novermber 10, 2021" />
  <title>HPC on Lawrencium Supercluster</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">HPC on Lawrencium Supercluster</h1>
  <p class="author">
Novermber 10, 2021
  </p>
  <p class="date">Wei Feinstein, Edison Lam, and HPCS Team</p>
</div>
<div id="introduction" class="slide section level1">
<h1>Introduction</h1>
<p>Slides and sample codes can be found on github <a
href="https://github.com/lbnl-science-it/Training-HPC-on-Lawrencium">https://github.com/lbnl-science-it/Training-HPC-on-Lawrencium</a></p>
<p>Video will be posted</p>
<p>There will be a hands-on session at the end of this training</p>
<p><a
href="https://docs.google.com/forms/d/e/1FAIpQLScX7D_OnHLVEqCJY1iGYQotfMMcxIX5SauF4c33ks08U3vhBw/viewform">Training
survey</a></p>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<ul>
<li>Overview of Lawrencium supercluster</li>
<li>Access to the cluster
<ul>
<li>Project types</li>
<li>User accounts</li>
<li>login</li>
<li>Storage/compute space (home, scratch, group, condo storage)</li>
</ul></li>
<li>Data transfer
<ul>
<li>DTN data transfer node</li>
<li>Globus</li>
<li>GDrive</li>
</ul></li>
<li>Software stack and installation
<ul>
<li>Software Module Farm</li>
<li>Installation of your own software</li>
</ul></li>
<li>Job submission and monitoring
<ul>
<li>Accounts &amp; partitions</li>
<li>Basic job submission</li>
<li>Interactive jobs</li>
<li>GPU job submission</li>
<li>Submit serial tasks in parallel</li>
</ul></li>
<li>Open Ondemand Web Service
<ul>
<li>Overview</li>
<li>Jupyter notebooks</li>
<li>Customized kernel</li>
<li>Remote visualization<br />
</li>
</ul></li>
<li>Hands-on exercises</li>
</ul>
</div>
<div id="lawrencium-cluster-overview" class="slide section level1">
<h1>Lawrencium Cluster Overview</h1>
<ul>
<li>A LBNL Condo Cluster Computing program
<ul>
<li>Support researchers in all disciplines at the Lab</li>
<li>Significant investment by the IT division</li>
<li>Individual PIs buy in compute nodes and storage</li>
<li>Computational cycles are shared among all lawrencium users</li>
</ul></li>
<li>Lawrencium Compute Nodes
<ul>
<li>data center is housed in the building 50B</li>
<li>1238 CPU Compute nodes, more than 37,192 cores</li>
<li>152 GPU cards</li>
<li>8 partitions, lr3, …, lr6, es1, cm1</li>
<li>~1300 user accounts</li>
<li>~530 groups</li>
</ul></li>
<li>Standalone Clusters</li>
</ul>
</div>
<div id="conceptual-diagram-of-lawrencium" class="slide section level1">
<h1>Conceptual Diagram of Lawrencium</h1>
<p><left><img src="figures/lrc1.png" width="80%"></left></p>
<p><a
href="https://it.lbl.gov/resource/hpc/supported-research-clusters/lawrencium/">Detailed
information of Lawrencium</a></p>
</div>
<div id="getting-access-to-lawrencium" class="slide section level1">
<h1>Getting Access to Lawrencium</h1>
<h4 id="three-types-of-project-accounts">Three types of Project
Accounts</h4>
<ul>
<li><em>Primary Investigator (PI) Computing Allowance (PCA)
account</em>: free 300K SUs per year (pc_xxx)</li>
<li><em>Condo account</em>: PIs buy in compute nodes to be added to the
general pool, in exchange for their own priority access and share the
Lawrencium infrastructure (lr_xxx)</li>
<li><em>Recharge account</em>: pay as you go with minimal recharge rate
~ $0.01/SU (ac_xxx)</li>
<li>Details about project accounts can be found <a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/">https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/</a></li>
<li><a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/accounts/projects-accounts">Request
Project Accounts</a></li>
<li>PIs can grant access researchers/students and external collaborators
to their PCA/Condo/Recharge Projects</li>
</ul>
<h4 id="user-accounts">User Accounts</h4>
<ul>
<li>PIs sponsor researchers/students and external collaborators for
cluster accounts</li>
<li><a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/accounts/user-accounts/">User
Account Request Form</a></li>
<li><a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/accounts/user-accounts/">User
Agreement Form</a></li>
</ul>
</div>
<div id="login-to-lawrencium-cluster" class="slide section level1">
<h1>Login to Lawrencium Cluster</h1>
<ul>
<li>Linux: Terminal (command-line) session.</li>
<li>Mac: Terminal (see Applications -&gt; Utilities -&gt;
Terminal).</li>
<li>Windows: PowerShell, or <a
href="https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html">PuTTY</a>.</li>
<li>One-time passwords (OTPs): set up your smartphone or tablet with
Google Authenticator with <a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/connecting/multi-factor-authentication">Instructions
here</a></li>
<li>Login:</li>
</ul>
<pre><code>ssh $USER@lrc-login.lbl.gov
password:</code></pre>
<ul>
<li><p>Password: your 4-digit PIN followed by the one-time password from
which your Google Authenticator app generates on your
phone/tablet.</p></li>
<li><p><strong>DO NOT run jobs on login nodes!!</strong></p></li>
</ul>
</div>
<div id="user-space" class="slide section level1">
<h1>User Space</h1>
<ul>
<li>Home: <code>/global/home/users/$USER/</code> 20GB per user, data is
backed up</li>
<li>Global scratch: <code>/global/scratch/$USER/</code>, shared, no
backup, where to launch jobs</li>
<li>Shared group project space
<ul>
<li>/global/home/groups-sw/ 200GB backup</li>
<li>/global/home/group/ 400GB no backup</li>
</ul></li>
<li>Condo storage:
<ul>
<li><code>e.g. /clusterfs/etna/ or /global/scratch/projects/xxx</code></li>
</ul></li>
</ul>
</div>
<div id="data-transfer" class="slide section level1">
<h1>Data Transfer</h1>
<h4 id="lrc-xfer.lbl.gov-data-transfer-node-dtn">lrc-xfer.lbl.gov: Data
Transfer Node (DTN)</h4>
<ul>
<li>On Linux: scp/rsync</li>
</ul>
<pre><code># Transfer data from a local machine to Lawrencium
scp file-xxx $USER@lrc-xfer.lbl.gov:/global/home/users/$USER
scp -r dir-xxx $USER@lrc-xfer.lbl.gov:/global/scratch/$USER

# Transfer from Lawrencium to a local machine
scp $USER@lrc-xfer.lbl.gov:/global/scratch/$USER/file-xxx ~/Desktop

# Transfer from Lawrencium to Another Institute
ssh $USER@lrc-xfer.lbl.gov   # DTN
scp -r $USER@lrc-xfer.lbl.gov:/file-on-lawrencium $USER@other-institute:/destination/path/$USER

rsync: a better data transfer tool as a backup tool
rsync -avpz file-at-local $USER@lrc-xfer.lbl.gov:/global/home/user/$USER</code></pre>
<ul>
<li>On Window
<ul>
<li><a href="https://winscp.net/eng/index.php">WinSCP</a>: SFTP client
and FTP client for Microsoft Windows</li>
<li><a href="https://filezilla-project.org/">FileZella</a>:
multi-platform program via SFTP</li>
</ul></li>
</ul>
</div>
<div id="data-transfer-with-globus" class="slide section level1">
<h1>Data Transfer with Globus</h1>
<ul>
<li>Globus lets you transfer and share data on your storage systems with
collaborators</li>
<li>Fast data transfer, refer to <a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/data-movement-and-storage/globus/">instructions</a></li>
<li>Berkeley Lab users can use Globus to transfer files in/out of their
LBNL Google drive. Details about Google drive via Globus is <a
href="https://commons.lbl.gov/display/itdivision/GDrive+Access+Via+Globus">here</a></li>
<li>Possible endpoints include: lbnl#lrc, your laptop/desktop, NERSC,
among others.</li>
<li>Transfer data to/from your laptop (endpoint setup)
<ul>
<li>Create an endpoint on your machine using Globus Connect Personal <a
href="https://www.globus.org/globus-connect-personal">https://www.globus.org/globus-connect-personal</a></li>
<li>Run Globus Connect Pesonal on your local machine</li>
</ul></li>
</ul>
<p><left><img src="figures/globus.jpg" width="60%"></left></p>
</div>
<div id="software-module-farm" class="slide section level1">
<h1>Software Module Farm</h1>
<ul>
<li>Software stack, commonly used compiler, software tools provided to
all cluster users</li>
<li>Installed and maintained on a centralized storage device and mounted
as read-only NFS file system
<ul>
<li>Compilers: e.g. intel, gcc, MPI compilers, Python</li>
<li>Tools: e.g.matlab, singularity, cuda</li>
<li>Applications: e.g. machine learning, QChem, MD, cp2k</li>
<li>Libraries: e.g. fftw, lapack</li>
</ul></li>
</ul>
<pre><code>[@n0000.scs00 ~]$ module avail
---- /global/software/sl-7.x86_64/modfiles/langs ----
gcc/6.3.0  intel/2016.4.072  python/2.7 python/3.5 cuda/9.0 julia/0.5.0 ...

---- /global/software/sl-7.x86_64/modfiles/tools ----
cmake/3.7.2  gnuplot/5.0.5  octave/4.2.0 matlab/r2017b(default)  ...

---- /global/software/sl-7.x86_64/modfiles/apps ----
bio/blast/2.6.0 math/octave/current ml/tensorflow/2.5.0-py37 ... 
...</code></pre>
</div>
<div id="environment-modules" class="slide section level1">
<h1>Environment Modules</h1>
<ul>
<li>Manage users’ software environment dynamically</li>
<li>Properly set up PATH, LD_LIBRARY_PATH…</li>
<li>Avoid clashes between incompatible software versions</li>
</ul>
<pre><code>module purge: clear user’s work environment
module available: check available software packages
module load xxx*: load a package
module list: check currently loaded software </code></pre>
<ul>
<li>Modules are arranged in a hierarchical fashion, some of the modules
become available only after the parent module(s) are loaded</li>
<li>e.g., MKL, FFT, and HDF5/NetCDF software is nested within the gcc
module</li>
<li>Example: load an OpenMPI package</li>
</ul>
<pre><code>module available openmpi mkl
module load intel/2016.4.072
module av openmpi
module load mkl/2016.4.072 openmpi/3.0.1-intel</code></pre>
<ul>
<li><a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/software-module-farm/">More
Environment Modules Information</a></li>
<li>Users are allowed to install software in their home or group
space</li>
<li>Users don’t have admin rights, but most software can be installed
<code>--prefix=/dir/to/your/path</code></li>
</ul>
</div>
<div id="install-python-packages" class="slide section level1">
<h1>Install Python Packages</h1>
<ul>
<li>Python modules: abundantly available but cannot be installed in the
default location without admin rights.</li>
<li><code>pip install --user package_name</code></li>
<li><code>export PYTHONPATH</code></li>
</ul>
<pre><code>[wfeinstein@n0000 ~]$ module available python
--------------------- /global/software/sl-7.x86_64/modfiles/langs -----------------------------------
python/2.7          python/3.5          python/3.6(default) python/3.7          python/3.7.6        python/3.8.2-dll
[wfeinstein@n0000 ~]$ module load python/3.7

[wfeinstein@n0000 ~]$ python3 -m site --user-site
/global/home/users/wfeinstein/.local/lib/python3.7/site-packages

[wfeinstein@n0000 ~]$ pip install --user ml-python
...
Successfully built ml-python
Installing collected packages: ml-python
Successfully installed ml-python-2.2

[wfeinstein@n0000 ~]$ export PYTHONPATH=~/.local/lib/python3.7/site-packages:$PYTHONPATH</code></pre>
<ul>
<li>pip install:
<code>--install-option="--prefix=$HOME/.local" package_name</code></li>
<li>Install from source code:
<code>python setup.py install –home=/home/user/package_dir</code></li>
<li>Creat a virtual environmemt: <code>python -m venv my_env</code></li>
<li>Isolated Python environment</li>
</ul>
</div>
<div id="slurm-resource-manager-job-scheduler"
class="slide section level1">
<h1>SLURM: Resource Manager &amp; Job Scheduler</h1>
<h3 id="overview">Overview</h3>
<p>SLURM is the resource manager and job scheduler to managing all the
jobs on the cluster</p>
<p>Why is this necessary?</p>
<ul>
<li>Prevent users’ jobs running on the same nodes.</li>
<li>Allow everyone to fairly share Lawrencium resources.</li>
</ul>
<p>Basic workflow:</p>
<ul>
<li>login to Lawrencium; you’ll end up on one of the login nodes in your
home directory</li>
<li>cd to the directory from which you want to submit the job
(scratch)</li>
<li>submit the job using sbatch (or an interactive job using srun,
discussed later)</li>
<li>SLURM assign compute node(s) to your jobs</li>
<li>your jobs will run on a compute node, not the login node</li>
</ul>
</div>
<div id="slurm-related-environment-variables"
class="slide section level1">
<h1>Slurm-Related Environment Variables</h1>
<ul>
<li><p>Slurm provides global variables</p></li>
<li><p>Can be used in your job submission scripts to adapt the resources
being requested in order to avoid hard-code</p></li>
<li><p>Examples of Slurm variables</p>
<ul>
<li>SLURM_WORKDIR</li>
<li>SLURM_NTASKS</li>
<li>SLURM_CPUS_PER_TASK</li>
<li>SLURM_CPUS_ON_NODE</li>
<li>SLURM_NODELIST</li>
<li>SLURM_NNODES</li>
</ul></li>
</ul>
</div>
<div id="accounts-partitions-quality-of-service-qos"
class="slide section level1">
<h1>Accounts, Partitions, Quality of Service (QOS)</h1>
<p>Check slurm association, such as qos, account, partition, the
information is required when submitting a job</p>
<pre><code>sacctmgr show association user=wfeinstein -p

Cluster|Account|User|Partition|Share|Priority|GrpJobs|GrpTRES|GrpSubmit|GrpWall|GrpTRESMins|MaxJobs|MaxTRES|MaxTRESPerNode|MaxSubmit|MaxWall|MaxTRESMins|QOS|Def QOS|GrpTRESRunMins|
perceus-00|pc_scs|wfeinstein|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|ac_test|wfeinstein|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|pc_test|wfeinstein|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|pc_test|wfeinstein|lr_bigmem|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|lr_test|wfeinstein|lr3|1||||||||||||condo_test|||
perceus-00|scs|wfeinstein|es1|1||||||||||||es_debug,es_lowprio,es_normal|||
...</code></pre>
<p>Lawrencium Cluster Info Click <a
href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/lbnl-supercluster/lawrencium">Here</a></p>
</div>
<div id="job-submission" class="slide section level1">
<h1>Job Submission</h1>
<h3 id="submit-an-interactive-job">Submit an Interactive Job</h3>
<p>Typically used for code debugging, testing, monitoring</p>
<ul>
<li><p>srun: add your resource request to the queue.</p></li>
<li><p>When the allocation starts, a new bash session will start up on
one of the granted nodes</p></li>
<li><p><code>srun --account=ac_xxx --nodes=1 --partition=lr5 --qos=lr_normal --time=1:0:0 --pty bash</code></p></li>
<li><p><code>srun -A ac_xxx -N 1 -p lr5 -q lr_normal -t 1:0:0 --pty bash</code></p></li>
</ul>
<pre><code>[wfeinstein@n0003 ~]$ srun --account=scs --nodes=1 --partition=lr6 --time=1:0:0 --qos=lr_normal --pty bash
srun: Granted job allocation 28755918
srun: Waiting for resource configuration
srun: Nodes n0101.lr6 are ready for job
[wfeinstein@n0101 ~]$ squeue -u wfeinstein
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          28755918       lr6     bash wfeinste  R       0:14      1 n0101.lr6</code></pre>
<p>Once you are on the assigned compute node, start application/commands
directly</p>
<ul>
<li>salloc: similarly to <em>srun –pty bash</em>.</li>
<li>a new bash session will start up on the login node</li>
</ul>
</div>
<div id="node-features" class="slide section level1">
<h1>Node Features</h1>
<p>Compute nodes may have different hardware within a SLURM partition,
e.g. LR6</p>
<ul>
<li><p>lr6_sky: Intel Skylak</p></li>
<li><p>lr6_cas: Intel Cascade Lake</p></li>
<li><p>lr6_cas,lr6_m192: lr6_cas + 192GB RAM</p></li>
<li><p>lr6_sky,lr6_m192: lr6_sky + 192GB RAM</p></li>
<li><p>When a specific type of node is requsted, wait time typically is
longer</p></li>
<li><p>Slurm flag: –constrain</p></li>
</ul>
<pre><code>[wfeinstein@n0000 ~]$ srun --account=scs --nodes=1 --partition=lr6 --time=1:0:0 --qos=lr_normal --constrain=lr6_sky --pty bash

[wfeinstein@n0081 ~]$ free -h
              total        used        free      shared  buff/cache   available
Mem:            93G        2.2G         83G        3.1G        7.4G         87G
Swap:          8.0G          0B        8.0G
[wfeinstein@n0081 ~]$ exit
exit
[wfeinstein@n0000 ~]$ srun --account=scs --nodes=1 --partition=lr6 --time=1:0:0 --qos=lr_normal --constrain=lr6_sky,lr6_m192 --pty bash
[wfeinstein@n0023 ~]$ free -h
              total        used        free      shared  buff/cache   available
Mem:           187G        2.6G        172G        1.7G         12G        182G
Swap:          8.0G        1.5G        6.5G</code></pre>
<ul>
<li>Node Features Can Be Found <a
href="https://it.lbl.gov/resource/hpc/supported-research-clusters/lawrencium/">Here</a></li>
</ul>
</div>
<div id="memory-specification" class="slide section level1">
<h1>Memory Specification</h1>
<ul>
<li><p>Most Lawrencium partitions are exclusive: a compute node allows
only one user</p></li>
<li><p>Some condo accounts or partitions, such as ES1 (GPUs), each
compute node can be shared by multiple users</p></li>
<li><p>Slurm flag: –mem (MB) is required when using a shared
partition:</p></li>
<li><p>e.g. a compute node with 96GB RAM, 40 core node: 2300
RAM/core</p>
<ul>
<li><code>--ntask=1 --mem=2300</code> (request one core)</li>
<li><code>--ntask=2 --mem=4600</code> (request 2 cores)</li>
</ul></li>
<li><p>LR6 partition lr_bigmem: two large memory nodes (1.5TB)</p></li>
<li><p>Slurm flag: –partition=lr_bigmem</p></li>
</ul>
</div>
<div id="submit-a-batch-job" class="slide section level1">
<h1>Submit a Batch Job</h1>
<ul>
<li>Get help with the complete command options
<code>sbatch --help</code></li>
<li>sbatch: submit a job to the batch queue system
<code>sbatch myjob.sh</code></li>
</ul>
<pre><code>#!/bin/bash -l

# Job name:
#SBATCH --job-name=mytest
#
# Partition:
#SBATCH --partition=lr6
#
# Account:
#SBATCH --account=pc_test
#
# qos:
#SBATCH --qos=lr_normal
#
# Wall clock time:
#SBATCH --time=1:00:00
#
# Node count
#SBATCH --nodes=1
#
# Node feature
#SBATCH --constrain=lr6_cas
#
#SBATCH --mail-user=xxx@lbl.gov
# email type
##SBATCH --mail-type=BEGIN/END/FAIL
#SBATCH --mail-type=ALL

# cd to your work directory
cd /your/dir

## Commands to run
module load python/3.7
python my.py &gt;&amp; mypy.out </code></pre>
</div>
<div id="submit-jobs-to-es1-gpu-partition" class="slide section level1">
<h1>Submit Jobs to ES1 GPU Partition</h1>
<h4 id="interactive-gpu-jobs">Interactive GPU Jobs</h4>
<ul>
<li><code>--gres=gpu:type:GPU#</code></li>
<li><code>--ntasks=CPU_CORE#</code></li>
<li>ratio CPU_CORE#:GPU# = 2:1</li>
</ul>
<pre><code>srun -A your_acct -N 1 -p es1 --gres=gpu:1 --ntasks=2 -q es_normal –t 0:30:0 --pty bash

[wfeinstein@n0000 ~]$ srun -A scs -N 1 -p es1 --gres=gpu:1 --ntasks=2 -q es_normal -t 0:30:0 --pty bash
[wfeinstein@n0019 ~]$ nvidia-smi
Sat Feb  6 10:13:25 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.44       Driver Version: 440.44       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   45C    P0    53W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |
| N/A   45C    P0    55W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+</code></pre>
<ul>
<li>Specify GPU type
<ul>
<li>GTX1080TI: –gres=gpu:GTX1080TI:1 (decommissioned)</li>
<li>GRTX2080TI: –gres=gpu:GRTX2080TI:1</li>
<li>V00: –gres=gpu:V100:1</li>
<li>A40: (6 2U A40 coming up)</li>
</ul></li>
</ul>
<pre><code>[wfeinstein@n0000 ~]$ srun -A scs -N 1 -p es1 --gres=gpu:V100:2 --ntasks=4 -q es_normal -t 0:30:0 --pty bash

[wfeinstein@n0016 ~]$ nvidia-smi -L
GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-7979861e-e0ad-000f-95fb-371e34593991)
GPU 1: Tesla V100-SXM2-16GB (UUID: GPU-50d24ac9-9eea-f96b-cc8b-db849f9c9427)

[wfeinstein@n0016 ~]$ echo $CUDA_VISIBLE_DEVICES
0,1</code></pre>
</div>
<div id="submit-a-gpu-batch-job" class="slide section level1">
<h1>Submit A GPU Batch Job</h1>
<p>Job Submission Script Example</p>
<pre><code>#!/bin/bash -l

#SBATCH --job-name=mytest
#SBATCH --partition=es1         ## es1 GPU partition
#SBATCH --account=pc_test
#SBATCH --qos=es_normal         ## qos of es1
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --gres=gpu:V100:2       ## GPUs
#SBATCH --ntasks=4              ## CPU cores
#
cd /your/dir

## Commands to run
module load python/3.7
python my.py &gt;&amp; mypy.out </code></pre>
</div>
<div id="submit-a-mpi-job" class="slide section level1">
<h1>Submit A MPI Job</h1>
<p>When use multiple nodes, you need to carefully specify the resources.
The key flags for use in your job script are:</p>
<ul>
<li>–nodes (or -N): number of nodes</li>
<li>–ntasks-per-node: number of tasks (i.e., processes) to run on each
node, especially useful when your job uses large memory, &lt; Max Core#
on a node</li>
<li>–cpus-per-task (or -c): number of CPUs to be used for each task</li>
<li>–ntasks (or -n): total number of tasks and let the scheduler
determine how many nodes and tasks per node are needed.</li>
<li>In general –cpus-per-task will be 1 except when running threaded
code.</li>
</ul>
<pre><code>#!/bin/bash -l

#SBATCH --job-name=myMPI
#SBATCH --partition=lr6
#SBATCH --account=scs
#SBATCH --qos=lr_normal
#SBATCH --time=2:00:00
#SBATCH --nodes=2                ## Nodes count
##SBATCH --ntasks=80             ## Number of total MPI tasks to launch (example):  
##SBATCH --ntasks-per-node=20    ## important with large memory requirement

cd /your/dir

## Commands to run
module load intel/2016.4.072 openmpi/3.0.1-intel
mpirun -np 80 ./my_mpi_exe        ## Launch your MPI application</code></pre>
</div>
<div id="submit-serial-tasks-in-parallel-gnu-parallel"
class="slide section level1">
<h1>Submit Serial Tasks in Parallel (GNU Parallel)</h1>
<p>GNU Parallel is a shell tool for executing jobs in parallel on one or
multiple computers.</p>
<ul>
<li>A job can be a single core serial task, multi-core or MPI
application.</li>
<li>A job can also be a command that reads from a pipe.</li>
<li>Typical input:
<ul>
<li>bash script for a single task</li>
<li>a list of tasks with parameters</li>
</ul></li>
</ul>
</div>
<div id="example-using-gnu-parallel" class="slide section level1">
<h1>Example Using GNU Parallel</h1>
<p>Bioinformatics tool <em>blastp</em> to compare 200 target protein
sequences against sequence DB</p>
<p>Serial bash script: <strong>run-blast.sh</strong></p>
<pre><code>#!/bin/bash
module load  bio/blast/2.6.0
blastp -query $1 -db ../blast/db/img_v400_PROT.00 -out $2  -outfmt 7 -max_target_seqs 10 -num_threads 1</code></pre>
<p><strong>task.lst</strong>: each line provides one parameter to one
task:</p>
<pre><code>[user@n0002 ]$ cat task.lst    
 ../blast/data/protein1.faa
 ../blast/data/protein2.faa
 ...
 ../blast/data/protein200.faa</code></pre>
<p>Instead submit single core-jobs 200 times, which potentially need 200
nodes, GNU parallel sends single-core jobs in parallel using all the
cores available, e.g. 2 compute nodes 32*2=64 parallel tasks. Once a CPU
core becomes available, another job will be sent to this resource.</p>
<pre><code>module load parallel/20200222
JOBS_PER_NODE=32
parallel --jobs $JOBS_PER_NODE --slf hostfile --wd $WDIR --joblog task.log --resume --progress \
                -a task.lst sh run-blast.sh {} output/{/.}.blst </code></pre>
<p>Detailed information of how to submit serial tasks in parallel with
<a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/running-jobs/gnu-parallel/">GNU
Parallel</a></p>
</div>
<div id="monitoring-jobs" class="slide section level1">
<h1>Monitoring Jobs</h1>
<ul>
<li>sinfo: check node status of a partition (idle, allocated, drain,
down)</li>
</ul>
<pre><code>[wfeinstein@n0000 ~]$ sinfo –r –p lr5
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
lr5          up   infinite      3 drain* n0004.lr5,n0032.lr5,n0169.lr5 
lr5          up   infinite     14   down n0048.lr5,n0050.lr5 
lr5          up   infinite     58  alloc n0000.lr5,n0001.lr5,n0002.lr5,n0003.lr5,n0006.lr5,n0009.lr5
lr5          up   infinite    115   idle n0005.lr5,n0007.lr5,n0008.lr5
...</code></pre>
<ul>
<li>squeue: check job status in the batch queuing system (R or PD)</li>
</ul>
<pre><code>squeue –u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) 
          28757187       lr6     bash wfeinste  R       0:09      1 n0215.lr6 
          28757723       es1     bash wfeinste  R       0:16      1 n0002.es1 
          28759191       lr6     bash wfeinste PD       0:00    120 (QOSMaxNodePerJobLimit)</code></pre>
<ul>
<li>sacct: check job information or history</li>
</ul>
<pre><code>[wfeinstein@n0002 ~]$ sacct -j 28757723
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
28757723           bash        es1        scs          2    RUNNING      0:0 

[wfeinstein@n0002 ~]$ sacct -X -o &#39;jobid,user,partition,nodelist,stat&#39;
       JobID      User  Partition        NodeList      State 
------------ --------- ---------- --------------- ---------- 
28755594     wfeinste+        lr5       n0192.lr5  COMPLETED 
28755597     wfeinste+        lr6       n0101.lr6  COMPLETED 
28755598     wfeinste+        lr5       n0192.lr5  COMPLETED 
28755604     wfeinste+ csd_lr6_s+       n0144.lr6  COMPLETED 
28755693     wfeinste+        lr6       n0101.lr6 CANCELLED+ 
....
28757187     wfeinste+        lr6       n0215.lr6  COMPLETED 
28757386     wfeinste+        es1       n0019.es1     FAILED 
28757389     wfeinste+        es1       n0002.es1    TIMEOUT 
28757723     wfeinste+        es1       n0002.es1    RUNNING </code></pre>
<ul>
<li><code>wwall -j &lt;JOB_ID&gt;</code>: check resouce utilization of
an active job from a login node</li>
</ul>
<pre><code>[wfeinstein@n0000 ~]$ wwall -j 28757187
--------------------------------------------------------------------------------
Total CPU utilization: 0%                          
          Total Nodes: 1         
               Living: 1                           Warewulf
          Unavailable: 0                      Cluster Statistics
             Disabled: 0                 http://warewulf.lbl.gov/
                Error: 0         
                 Dead: 0         
--------------------------------------------------------------------------------
 Node      Cluster        CPU       Memory (MB)      Swap (MB)      Current
 Name       Name       [util/num] [% used/total]   [% used/total]   Status
n0215.lr6               0%   (40) % 3473/192058    % 1655/8191      READY</code></pre>
<ul>
<li><code>scancel &lt;jobID&gt;</code> : scancel a job</li>
</ul>
<p>More Information of <a
href="https://it.lbl.gov/resource/hpc/for-users/hpc-documentation/running-jobs/">Slurm
Usage</a></p>
</div>
<div id="open-ondemand" class="slide section level1">
<h1>Open OnDemand</h1>
<ul>
<li>Single web point of entry to the Lawrencium Supercluster</li>
<li>Allow access to Lawrencium compute resources
<ul>
<li>File browser: file editing, data transfer</li>
<li>Shell command line access - terminal</li>
</ul></li>
<li>Monitor jobs</li>
<li>Interactive applications: Jupyter notebooks, MatLab, RStudio…</li>
<li>Jupyter server
<ul>
<li>Interactive mode: debugging code, light-weight visulization with 4
CPU nodes and 1 GPU node</li>
<li>Compute mode: Access to all Lawrencium partitions via submitting
batch jobs</li>
</ul></li>
<li>Sever: <a
href="https://lrc-ondemand.lbl.gov/">https://lrc-ondemand.lbl.gov/</a>
<ul>
<li>Intel Xeon Gold processor with 32 cores, 96 GB RAM</li>
</ul></li>
</ul>
</div>
<div id="open-ondemand-1" class="slide section level1">
<h1>Open Ondemand</h1>
<p><left><img src="figures/ood.png" width="70%"></left></p>
</div>
<div id="jupyter-notebook" class="slide section level1">
<h1>Jupyter Notebook</h1>
<ul>
<li>Create user kernels</li>
</ul>
<pre><code>python -m venv --system-site-packages ./mykernel
source ./mykernel/bin/activate
python -m ipykernel install --user --name=mykernel</code></pre>
<pre><code>[wfeinstein@n0000 ~]$ module load python/3.7
[wfeinstein@n0000 ~]$ module list
Currently Loaded Modulefiles:
  1) emacs/25.1   2) python/3.7
[wfeinstein@n0000 ~]$ python -m venv --system-site-packages ./mykernel
[wfeinstein@n0000 ~]$ source ./mykernel/bin/activate
(mykernel) [wfeinstein@n0000 ~]$ python -m ipykernel install --user --name=mykernel
Installed kernelspec mykernel in /global/home/users/wfeinstein/.local/share/jupyter/kernels/mykernel
(mykernel) [wfeinstein@n0000 ~]$    

# Now you should be able to choose the virtual environment &quot;mykernel&quot; as a kernel in Jupyter</code></pre>
</div>
<div id="one-minute-demo-launching-jupyter-notebooks"
class="slide section level1">
<h1>One-Minute Demo Launching Jupyter Notebooks</h1>
<p><a
href="https://lrc-ondemand.lbl.gov/">https://lrc-ondemand.lbl.gov/</a></p>
</div>
<div id="remote-visulization" class="slide section level1">
<h1>Remote Visulization</h1>
<ul>
<li><p>Allow users to run a real desktop within the cluster
environment</p></li>
<li><p>Allow applications with a GUI, commercial applications, debugger
or visualization applications to render results.</p>
<ul>
<li><p>Remote Desktop launched within Open OnDemand - <strong>coming up,
stay tuned</strong></p></li>
<li><p>viz node lrc-viz.lbl.gov</p></li>
<li><p>RealVNC is provided as the remote desktop service with local VNC
Viewer<br />
</p></li>
<li><p>Start VNC service on viz node lrc-viz.lbl.gov</p></li>
<li><p>Connect to the VNC server with VNC Viewer locally</p></li>
<li><p>Start applications: Firefox, Jupyter notebooks, paraview
…</p></li>
</ul></li>
</ul>
</div>
<div id="getting-help" class="slide section level1">
<h1>Getting help</h1>
<ul>
<li>Virtual Office Hours:
<ul>
<li>Time: 10:30am - noon (Wednesdays)</li>
<li>Online <a
href="https://docs.google.com/forms/d/e/1FAIpQLScBbNcr0CbhWs8oyrQ0pKLmLObQMFmYseHtrvyLfOAoIInyVA/viewform">Request</a></li>
</ul></li>
<li>Sending us tickets at hpcshelp@lbl.gov</li>
<li>More information, documents, tips of how to use LBNL Supercluster <a
href="http://scs.lbl.gov">http://scs.lbl.gov/</a></li>
<li>New Science IT website will be launched Nov 15th, 2021</li>
<li>Please fill out <a
href="https://docs.google.com/forms/d/e/1FAIpQLScX7D_OnHLVEqCJY1iGYQotfMMcxIX5SauF4c33ks08U3vhBw/viewform">Training
Survey</a></li>
</ul>
</div>
<div id="hands-on-exercise" class="slide section level1">
<h1>Hands-on Exercise</h1>
<ol style="list-style-type: decimal">
<li>Login and data transfer</li>
<li>Set up work environment using module commands</li>
<li>Edit files</li>
<li>Submit jobs</li>
<li>Monitor jobs</li>
</ol>
</div>
<div id="login-and-data-transfer" class="slide section level1">
<h1>Login and Data Transfer</h1>
<p>Objective: transfer data to/from LRC</p>
<ol style="list-style-type: decimal">
<li><p>Download test data <a href="data.sample">here</a></p></li>
<li><p>Open two linux terminals on Mac or Window via Putty</p></li>
<li><p>Transfer local data.sample to LRC on terminal 1</p></li>
</ol>
<pre><code>scp -r data.sample $USER@lrc-xfer.lbl.gov:/global/home/users/$USER 
scp -r data.sample $USER@lrc-xfer.lbl.gov:~</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>On terminal 2, login to LRC</li>
</ol>
<pre><code>ssh $USER@lrc-login.lbl.gov 
pwd 
cat data.sample
cp data.sample data.bak</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Transfer data from LRC DTN to your local machine on terminal 1</li>
</ol>
<pre><code>scp -r $USER@lrc-xfer.lbl.gov:/global/home/users/$USER/data.bak .
ls data.*</code></pre>
</div>
<div id="module-commands" class="slide section level1">
<h1>Module Commands</h1>
<ul>
<li>Display software packages on LRC <code>module available</code></li>
<li>Check modules in your env <code>module list</code></li>
<li>Clear your env <code>module purge</code></li>
<li>Load a module</li>
</ul>
<pre><code> module available openmpi mkl
 module load intel/2016.4.072
 module list
 module av openmpi mkl
 module load mkl/2016.4.072 openmpi/3.0.1-intel</code></pre>
</div>
<div id="editing-files" class="slide section level1">
<h1>Editing files</h1>
<p>Linux editor: vim and emacs installed. Just start the editor from a
login node.</p>
<pre><code>## To use vim
vim myfile.txt
## To use emacs
emacs myfile.txt</code></pre>
</div>
<div id="job-submission-1" class="slide section level1">
<h1>Job Submission</h1>
<ul>
<li>Check your account slurm association</li>
</ul>
<pre><code>sacctmgr show association -p user=$USER

Cluster|Account|User|Partition|Share|Priority|GrpJobs|GrpTRES|GrpSubmit|GrpWall|GrpTRESMins|MaxJobs|MaxTRES|MaxTRESPerNode|MaxSubmit|MaxWall|MaxTRESMins|QOS|Def QOS|GrpTRESRunMins|
perceus-00|scs|wfeinstein|lr6|1|||||||||||||lr6_lowprio,lr_debug,lr_normal|||
perceus-00|scs|wfeinstein|es1|1|||||||||||||es_debug,es_lowprio,es_normal|||
</code></pre>
<h3 id="request-an-interactive-node">Request an interactive node</h3>
<p>Note: Use your account, partition, qos</p>
<p>srun –account=ac_xxx –nodes=1 –partition=xxx –time=1:0:0 –qos=xxx
–pty bash</p>
</div>
<div id="submit-a-batch-job-1" class="slide section level1">
<h1>Submit a batch job</h1>
<p>Download a sample <a href="my_submit.sh">job submission script</a>
and <a href="my.py">python sample</a></p>
<p>Note: Use your account, partition, qos</p>
<pre><code>#!/bin/bash -l

# Job name:
#SBATCH --job-name=mytest
#
# Partition:
#SBATCH --partition=lr6
#
# Account:
#SBATCH --account=your_account
#
# qos:
#SBATCH --qos=lr_normal
#
# Wall clock time:
#SBATCH --time=1:00:00
#
# Node count
#SBATCH --nodes=1
#
# Node feature
##SBATCH --constrain=lr6_cas
#
# cd to your work directory
cd /global/scratch/$USER

## Commands to run
module load python/3.7
python my.py &gt;&amp; mypy.out </code></pre>
</div>
<div id="monitor-jobs" class="slide section level1">
<h1>Monitor jobs</h1>
<p><code>squeue -u $USER</code></p>
<p><code>sacct -j &lt;JOBID&gt;</code></p>
<p><code>wwall -j &lt;JOBID&gt;</code></p>
</div>
</body>
</html>
